{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 1. [Probability Models and Axioms](http://www.youtube.com/watch?v=j9WZyLZCBzs&list=SPUl4u3cNGP61MdtwGTqZA0MreSaDybji8)\n",
      "\n",
      "- You have to read the book, the words are important to get intuition, rather than remember the formulas.\n",
      "\n",
      "- The __sample space__ $\\Omega$ is the set of all posible outcomes of an experiment.\n",
      "    - Collectively exhaustive\n",
      "    - Mutually exclusive\n",
      "    - This doesn't make $\\Omega$ unique. Flip coin and rain example. _Art: to be ar the right granularity_\n",
      "    \n",
      "- Probability axioms\n",
      "    - __Event:__ a subset of the sample space\n",
      "    - We __asign probabilities to events__ according to our __beliefs__ about their likelihood: how often they occur.\n",
      "    - Axioms:\n",
      "        1. ___Nonnegativity:___ $P(A) \\geqslant 0$\n",
      "        2. ___Normalization:___ $P(\\Omega) = 1$\n",
      "        3. ___Additivity:___ If $A \\cap B = \\emptyset$, then $P(A \\cup B) = P(A) + P(B)$\n",
      "        4. _Countable Additivity:_ Additivity of the probabilities of an infinite sequence of disjoint events. Related to axiom of choice?\n",
      "        \n",
      "> Think of probability as cream cheese\n",
      "\n",
      "- There are weird sets which can't be assigned a probability under these axioms. $\\mathbb{R} \\setminus \\mathbb{Q}$, $\\mathbb{Q}$, compactness necessary?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 2. [Conditioning and Bayes' Rule](http://www.youtube.com/watch?v=TluTv5V0RmE&list=SPUl4u3cNGP61MdtwGTqZA0MreSaDybji8&feature=share&index=1)\n",
      "\n",
      "- Conditional probability\n",
      "- $P(A|B)$ is the probability of A given that B occurred. B is our new universe.\n",
      "- $P(A|B) = \\frac {P(A \\cap B)} {P(B)} = \\frac {P(A) P(B|A)} {P(B)}$\n",
      "- Plane and radar example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 3. [Independence](http://www.youtube.com/watch?v=19Ql_Q3l0GA&feature=share&list=SPUl4u3cNGP61MdtwGTqZA0MreSaDybji8&index=2)\n",
      "\n",
      "- Definition of independence: $P(B|A) = P(B) \\iff $ A and B are independent.\n",
      "- Another almost equivalent defintion: $P(A \\cap B) = P(A) P(B) \\iff$ A and B are independent.\n",
      "- Conditioning may affect independence.\n",
      "- A collection of independent events is one such that $P(\\bigcap A_i) = \\prod P(A_i)$ for all subcollections of $A_i$'s in the collection."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 4. Counting\n",
      "\n",
      "- All elements in $\\Omega$ have $P = 1/|\\Omega| = 1/N$. Thus if there are $n$ of them in an event, its probability is $n/N$. _Counting_ is concerned with finding $n$ and $N$.\n",
      "- Binomial coefficients and probabilities\n",
      "- $\\sum_{k=0}^{n}{{n}\\choose{k}} = 2^n$ this is true because we are counting all the ways to choose subsets from a set with $n$ elements, and this is equivalent to asigning a $0$ or $1$ to each element, which can be done in $2^n$ ways.\n",
      "- Relatedly, $\\sum_{k=0}^{n}{{{n}\\choose{k}} p^{k} {(1-p)}^{(n-k)}} = 1$.\n",
      "\n",
      "No escuch\u00e9 un carajo, pero es m\u00e1s o menos lo del pr\u00e1ctico 1 de la fing."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "# 5. Discrete Random Variables; Probability Mass Functions; Expectations\n",
      "\n",
      "# 6. Discrete Random Variable Examples; Joint PMFs\n",
      "\n",
      "# 7. Multiple Discrete Random Variables: Expectations, Conditioning, Independence\n",
      "\n",
      "# 8. Continuous Random Variables\n",
      "\n",
      "# 9. Multiple Continuous Random Variables\n",
      "\n",
      "# 10. Continuous Bayes' Rule; Derived Distributions\n",
      "\n",
      "# 11. Derived Distributions; Convolution; Covariance and Correlation\n",
      "\n",
      "# 12. Iterated Expectations; Sum of a Random Number of Random Variables\n",
      "\n",
      "# 13. Bernoulli Process\n",
      "\n",
      "# 14. Poisson Process I\n",
      "\n",
      "# 15. Poisson Process II\n",
      "\n",
      "# 16. Markov Chains I\n",
      "\n",
      "# 17. Markov Chains II\n",
      "\n",
      "# 18. Markov Chains III\n",
      "\n",
      "# 19. Weak Law of Large Numbers\n",
      "\n",
      "# 20. Central Limit Theorem\n",
      "\n",
      "# 21. Bayesian Statistical Inference I\n",
      "\n",
      "# 22. Bayesian Statistical Inference II\n",
      "\n",
      "# 23. Classical Statistical Inference I\n",
      "\n",
      "# 24. Classical Inference II\n",
      "\n",
      "# 25. Classical Inference III; Course Overview"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}